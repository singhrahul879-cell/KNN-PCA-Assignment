{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?"
      ],
      "metadata": {
        "id": "wEMZKJWZEV_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - K-Nearest Neighbors (KNN) is a supervised, non-parametric, instance-based learning algorithm.\n",
        "\n",
        "**How it works**\n",
        "\n",
        "Choose a value of K (number of neighbors).\n",
        "\n",
        "Compute the distance between the test point and all training points.\n",
        "\n",
        "Select the K nearest neighbors.\n",
        "\n",
        "Make a prediction based on neighbors.\n",
        "\n",
        "**Classification**\n",
        "\n",
        "Output = majority class among K neighbors.\n",
        "\n",
        "Example: If 3 out of 5 neighbors belong to Class A → predict Class A.\n",
        "\n",
        "**Regression**\n",
        "\n",
        "Output = average (or weighted average) of neighbor values."
      ],
      "metadata": {
        "id": "Ib3i6HFyF29d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the Curse of Dimensionality and how does it affect KNN performance?"
      ],
      "metadata": {
        "id": "HeEswFjQEZKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - The Curse of Dimensionality refers to problems that arise as the number of features increases.\n",
        "\n",
        "**Effect on KNN**\n",
        "\n",
        "Distance between points becomes less meaningful\n",
        "\n",
        "Neighbors become almost equally distant\n",
        "\n",
        "Model accuracy decreases\n",
        "\n",
        "Computation becomes expensive\n",
        "\n",
        "KNN performs poorly in high-dimensional spaces, making dimensionality reduction essential."
      ],
      "metadata": {
        "id": "z-j_VOUvGAD9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is Principal Component Analysis (PCA)? How is it different from feature selection?"
      ],
      "metadata": {
        "id": "jf7G0CfEEaT9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Principal Component Analysis (PCA) is an unsupervised dimensionality reduction technique.\n",
        "\n",
        "**What PCA does**\n",
        "\n",
        "Transforms original features into new orthogonal features (principal components)\n",
        "\n",
        "Captures maximum variance with fewer dimensions\n",
        "| Feature Selection                   | PCA                           |\n",
        "| ----------------------------------- | ----------------------------- |\n",
        "| Selects subset of original features | Creates new features          |\n",
        "| Keeps interpretability              | Loses direct interpretability |\n",
        "| Supervised or unsupervised          | Unsupervised                  |\n"
      ],
      "metadata": {
        "id": "maOzFZ6kGHJN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are eigenvalues and eigenvectors in PCA, and why are they important?"
      ],
      "metadata": {
        "id": "UmcrmRqREbXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - **Eigenvectors** → Directions of maximum variance (principal components)\n",
        "\n",
        " - **Eigenvalues** → Amount of variance captured by each eigenvector\n",
        "\n",
        "**Why important**\n",
        "\n",
        "Eigenvectors define the new feature space\n",
        "\n",
        "Eigenvalues help decide how many components to keep"
      ],
      "metadata": {
        "id": "E5E4MxHbGUu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: How do KNN and PCA complement each other when applied in a single pipeline?\n",
        "Dataset: Use the Wine Dataset from sklearn.datasets.load_wine()."
      ],
      "metadata": {
        "id": "vAPDVnEZEcwd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - PCA reduces dimensionality → solves curse of dimensionality\n",
        "\n",
        " - KNN becomes:\n",
        "\n",
        " - Faster\n",
        "\n",
        " - More accurate\n",
        "\n",
        " - Less noisy\n",
        "\n",
        "PCA improves distance quality, which is critical for KNN."
      ],
      "metadata": {
        "id": "oPRfMU8wGdhl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases."
      ],
      "metadata": {
        "id": "JmZkFvWDEeL-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Model                   | Accuracy   |\n",
        "| ----------------------- | ---------- |\n",
        "| KNN without scaling     | **72.22%** |\n",
        "| KNN with StandardScaler | **94.44%** |\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "KNN is highly sensitive to feature scale. Scaling dramatically improves performance."
      ],
      "metadata": {
        "id": "m3Gcqi-pGmDt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component."
      ],
      "metadata": {
        "id": "cqJxdYqrEkBm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - First few components capture most variance\n",
        "\n",
        " - Example insight:\n",
        "\n",
        " - PC1 ≈ highest variance\n",
        "\n",
        " - PC2 + PC1 together capture majority of information\n",
        "\n",
        "This confirms PCA is effective for dimensionality reduction."
      ],
      "metadata": {
        "id": "QNAM3gOCGs3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset."
      ],
      "metadata": {
        "id": "2m9k7olWElv2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Dataset            | Accuracy   |\n",
        "| ------------------ | ---------- |\n",
        "| Original (scaled)  | **94.44%** |\n",
        "| PCA (2 components) | **94.44%** |\n",
        "\n",
        "**Observation**\n",
        "\n",
        "Same accuracy with far fewer features\n",
        "\n",
        "Faster computation and better generalization"
      ],
      "metadata": {
        "id": "TvvCou4WGy2l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results."
      ],
      "metadata": {
        "id": "L0-zIs9VEm8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Distance Metric | Accuracy   |\n",
        "| --------------- | ---------- |\n",
        "| Euclidean       | **94.44%** |\n",
        "| Manhattan       | **98.15%** |\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "Manhattan distance performs better here\n",
        "\n",
        "Distance metric choice significantly impacts KNN"
      ],
      "metadata": {
        "id": "SZOpQqmEG7p9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working with a high-dimensional gene expression dataset to classify patients with different types of cancer.\n",
        "\n",
        "Due to the large number of features and a small number of samples, traditional models overfit.\n",
        "\n",
        "Explain how you would:\n",
        "\n",
        "● Use PCA to reduce dimensionality\n",
        "\n",
        "● Decide how many components to keep\n",
        "\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "\n",
        "● Evaluate the model\n",
        "\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world biomedical data\n"
      ],
      "metadata": {
        "id": "3mG0gjhbEsrV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Step-by-step solution\n",
        "\n",
        "**1. Use PCA**\n",
        "\n",
        "Reduce thousands of genes to fewer components\n",
        "\n",
        "Remove noise and correlated features\n",
        "\n",
        "**2. Decide number of components**\n",
        "\n",
        "Use explained variance ratio\n",
        "\n",
        "Keep components explaining 90–95% variance\n",
        "\n",
        "Use scree plot / cumulative variance\n",
        "\n",
        "**3. Apply KNN**\n",
        "\n",
        "Train KNN on PCA-reduced data\n",
        "\n",
        "Choose optimal K via cross-validation\n",
        "\n",
        "**4. Evaluate the model**\n",
        "\n",
        "Accuracy, Precision, Recall, F1-score\n",
        "\n",
        "Confusion Matrix\n",
        "\n",
        "Cross-validation for robustness\n",
        "\n",
        "**5. Justification to stakeholders**\n",
        "\n",
        "Prevents overfitting\n",
        "\n",
        "Handles small-sample, high-feature biomedical data\n",
        "\n",
        "Improves generalization\n",
        "\n",
        "Computationally efficient\n",
        "\n",
        "Clinically reliable and interpretable"
      ],
      "metadata": {
        "id": "RcLJEfciHDJt"
      }
    }
  ]
}